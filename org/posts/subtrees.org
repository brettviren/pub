#+hugo_base_dir: ../../
#+hugo_section: posts/
#+hugo_weight: auto
#+hugo_auto_set_lastmod: nil

#+author: Brett Viren

* Ideas                                                              :@ideas:
** DONE Unix Philosophy and the Wire-Cell Toolkit
CLOSED: [2022-06-11 Sat 10:23]
:PROPERTIES:
:EXPORT_FILE_NAME: unix-philosophy
:END:

For many years I have understood the [[https://en.wikipedia.org/wiki/Unix_philosophy]["Unix philosophy"]] of software to mean "do one thing and do it well".  Reading that Wikipedia entry teaches me that the original paper gives three more points.  I ponder these in relationship to the [[https://wirecell.github.io][Wire-Cell Toolkit]].

#+hugo: more

The "do one thing" is conceptually easy to grasp and is at the core of the Wire-Cell Toolkit (WCT).  Excluding some low-level utilities, "everything" is accessed via an abstract "interface" base class.  Each interface defines a small number of methods.  A developer creates a "component class" which is a concrete implementation of one or more interfaces.  User code, which could also be component code, can access an instance of a component via one of its interfaces.  Given that, the developer of user code need only worry about understanding a small set of semantic context to use the interface.

For example, the ~IConfigurable~ has two methods ~default_configuration()~ and ~configure()~.  The component expects the first to be called, its return value potentially modified and the result passed to the second call.  It also expects these two calls to occur in the same thread.  Whatever else may happen externally, with these simple rules assumed, the component developer is secure in coding what they need.  Likewise, interface-using code is free to do whatever it wants as long as these simple rules are followed.
These behavior rules may be likened to how Unix commands generally assume ample system memory and disk space, existence of input files, output directories, etc.

The Unix philosophy also requires that the many "one things" can be composed into novel, compound "one things".  As a corollary it constrains the information exchanged between the "one things" to take minimal and standardized form.  

Generalized, this composition is precisely a data flow graph and that is the primary (but not only) execution pattern followed by WCT applications.  In Unix we generally make only linear pipelines, if we make any compounds at all.  In some rare cases we may make moderately more complex graphs via ~tee~ or explicit use of higher numbered file descriptors.  The problems that WCT tackle are inherently much more complex than typically seen on the Unix command line and thus graphs become both broad (many pipelines) and deep (long pipelines).  This motivates WCT to use a more general "graph first" configuration language which is rather different than the "node first" or at most "pipeline first" semantics that Unix shell languages encourage.

The maxim covering minimal and standardized form of information exchange addresses the nature of graph edges.  In WCT we define an edge by a data interface abstract base class (~IData~).  This provides the /standardization/.  If one graph node port produces an ~IFrame~ the connected port must accept it and the receiving node knows precisely the form it is getting.  The /minimal/ criteria is less constrained.  Here, developers of data interfaces must think carefully how to factor the data structure concepts and anticipate not just immediate but future use.  For sure, careful design of ~IData~ is a cusp.  Get it right and the future is bright.  Get it wrong and the pain will be felt for a long time.  The uncharitable "keep it simple, stupid" slogan applies.  Found in hindsight there are existing cases where the slogan was violated and it has led to ongoing problems.  Yet, generally the intention of ~IData~ is exactly coherent with the philosophy.

The third maxim of the Unix philosophy embraces competition between alternative implementations.  The standardization of data exchange formats is the "market" that allows this competition.  One may take a compound graph and "snip" out a node or subgraph, replace it with a competitor and the result is the "same but different" job.  If the replacement allows faster, more accurate, less resource intensive or otherwise better results, the replacement wins.  Otherwise, we go back to the original, no harm, no foul.  The WCT configuration language allows such A/B testing to be easily performed.

Competition at the microscopic, graph node level is encouraged by support for completion at the macroscopic, library level.  The WCT plugin system allows developers to provide a shared library of WCT components in a manner of their choice, depending only on WCT's core "interface" library.  Developers who do not wish to invent their own project organization may produce WCT style packages easily either by hand or bootstrapping with the template-based code generator to make a [[https://github.com/brettviren/moo/tree/master/wcup][Wire-Cell User Package]] (WCUP).

The third maxim also encourage discarding of "clumsy parts".  Coupling the parts through explicit interface classes simplifies doing just that.  In addition, the WCT provides virtually all of the "batteries" needed to compose almost all jobs.  Only a small number of niche components needed to connect WCT graphs to external software are kept outside of the WCT code base.  This code centralization, sometimes called "monorepo", allows WCT developers to make sweeping changes when needed without involving and disrupting WCT users.

A recent example was the addition of the ~IDFT~ interface and component implementations which factors out discrete Fourier transform operations.  Previously, DFT functions were hard-linked in the WCT util library.  Moving them behind an interface now allows different ~IDFT~ implementations.  Already, WCT has gained ~IDFT~ implementations based on FFTW3 and PyTorch (CPU or GPU) and soon will merge in a direct CUDA (GPU) implementation.  The user with GPU resources can now accelerate every WCT component that uses DFTs with a simple configuration change and not C++ development.  However, in order to migrate from hard-linked to interface-based DFT a lot of C++ code had to be rewritten.  Since this code was all in the single WCT repository, the change was largely invisible to external user code that depends on WCT via its interfaces.

The last maxim of the philosophy is about programmatic automation.  Do not ask the human to do what software can.  The WCUP code generator is one example, though not yet widely used given the monorepo nature of mainline WCT development.  The factoring of functionality into components is another example.  WCT encourages a developer not to rewrite something which a component provides.

The WCT ~aux~ sub-package and library provides high-level code which may use other components and which components may hard-link so that they need not all solve the same problems.  For example, the ~IDFT~ interface types are simple C-style arrays.  Especially for 2D, these are not convenient to use in code.  Developers wish to use ~std::vector~ and Eigen3 arrays.  Thus the ~aux~ package provides the ~DftTools~ family of functions that adapt these hard-compiled types to the more general ~IDFT~.

Very recently, new developments related to the modeling and generation of noise has uncovered a new target for such factoring.  A future post here or at the [[https://wirecell.github.io/news/][Wire-Cell News]] will likely cover it.  In short, initial problems related to a particular type of noise were solved in one specific node implementation.  Support for new types of noise began to be added and that led to attempts to yet again solve these problems in new, redundant code.  To make for easy development by humans and more robust code WCT is factoring these common noise to shared tools.

I have no real conclusion to all this other than it satisfies my desire to express the parallels between the Unix philosophy and the WCT design.  Until bumping into the linked Wikipedia page, I was not aware of the maxims beyond the first.  Perhaps long time use of Unix caused them to seep into my thinking.  Or, perhaps, these maxims are just so obviously The Right Way To Do Things that they get honored without them needing to be explicitly stated!





* Tools                                                              :@tools:

** DONE Babel in Jsonnet with ~ob-jsonnet.el~               :emacs:jsonnet:org:
SCHEDULED: <2021-10-08 Fri> 
:PROPERTIES:
:EXPORT_FILE_NAME: ob-jsonnet
:EXPORT_DATE: 
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: 
:END:

I made some ~org-babel~ functions for Jsonnet and it was pretty easy!

#+hugo: more

[[https://git.sr.ht/~bzg/worg/tree/master/item/org-contrib/babel/ob-template.el][ob-template.el]] is a great starting point and made for light work.

*** Install

Download [[https://raw.githubusercontent.com/brettviren/ob-jsonnet/master/ob-jsonnet.el][~ob-jsonnet.el~]] to somewhere in your load path and add

#+begin_src lisp
(require 'ob-jsonnet)
#+end_src

And append ~(jsonnet . t)~ to ~org-babel-load-languages~.

You may customize ~org-babel-jsonnet-command~ if the ~jsonnet~ command is not in your ~$PATH~.

*** Examples

Basic usage

#+begin_example
  ,#+begin_src jsonnet :exports both :wrap "src json"
  {a:42}
  ,#+end_src
#+end_example

#+begin_src jsonnet :exports results :wrap "src json"
{a:42}
#+end_src

#+RESULTS:
#+begin_src json
{
   "a": 42
}
#+end_src

You can also supply command line options:

#+begin_example
  ,#+begin_src jsonnet :cmdline "-S" :exports both :wrap "example"
  std.join(" ", ["hello","world"])
  ,#+end_src
#+end_example

#+begin_src jsonnet :cmdline "-S" :exports results :wrap "example"
std.join(" ", ["hello","world"])
#+end_src

#+RESULTS:
#+begin_example
"hello world"
#+end_example

Now, if only we had proper syntax highlighting for Jsonnet in Hugo.



* Learning                                                        :@learning:


** DONE Vaccine Effectiveness                                   :covid:stats:
SCHEDULED: <2022-01-05 Wed>
:PROPERTIES:
:EXPORT_FILE_NAME: vaccine-effectiveness
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :math true
:END:

I have seen the quantitiy /vaccine effectiveness/ (VE) used in many
contexts and never really knew what it meant.  Today, I checked and
this is what I learned.  As I sort of expected, the causal connection
with the workings of the vaccine is not complete.  It leaves some room
to be influenced things that are merely correlated with being
vaccinated.  

#+hugo: more

NY state gov has some good info on [[https://coronavirus.health.ny.gov/covid-19-breakthrough-data][vaccine breakthrough]].  Currently
4.9% of vaxxed NYers caught COVID.  If you are vaxxed and catch it,
you'll contribute to only 0.15% of hospitalized COVID patients.  On
the topic of hospitalization, the page continues with:

#+begin_quote
For the week of May 3, 2021, the estimated vaccine effectiveness shows
fully-vaccinated New Yorkers had a 92.4% lower chance of becoming a
COVID-19 case, compared to unvaccinated New Yorkers.
#+end_quote

Well, 92.4 seems like a nice happy big number, great!...  But then,
"%-lower chance", that just sounds weird to me.  What does it mean?

That page conveniently links to the open article [[https://www.nejm.org/doi/full/10.1056/NEJMoa2116063][Rosenberg et all, Dec
2021]] which conveniently links to the even more useful [[https://www.nejm.org/doi/suppl/10.1056/NEJMoa2116063/suppl_file/nejmoa2116063_appendix.pdf][appendix]] which 
defines 

\[HR = \frac{h_{vaxxed}(t)}{h_{nasty}(t)}\]

Okay, those are my subscripts.  The $h(t)$ here is the called the
/hazard function/ which is apparently a term-of-art.  To the google!
That turns up these very clear and concise [[https://web.stanford.edu/~lutian/coursepdf/unit1.pdf][note]] and [[https://web.stanford.edu/~lutian/coursepdf/slideweek1.pdf][presentation]] from
a Stanford bio class.

There is explained that the /hazard function/ is a conditional
probability that some "event" (catching COVID) will occur for a time
$T \in [t,t+dt]$ given that the event had yet to occur by time $T=t$.

For example, a daily hazard function can tell us the probability we
get COVID tomorrow given we don't have it today. (Maybe it should be
called the Wimpy Hamburger Function).

The Stanford continues to define a /survival function/ and various
relations between it, a /cumulative hazard function/, the original PDF
and the hazard function.  But, I leave it there and go back to the NY
State report.  

There it gives this example:

- cohort size :: $N_c=215,159$ vaxxed people at risk in first week of May
- infected :: $N_i = 56$ number of vaxxed infected in first week of May

It defines the hazard function for that cohort that week as

\[h(t) = \frac{N_i}{N_c - \frac{N_i}{2}}\] 

where the $\frac{1}{2}$ in the denominator is apparently an attempt to
place the measure at the middle of the week (?).  Ie, half the
infected are removed from cohort.  That looks a little weird to me,
but it doesn't change the result much as long as $N_i \ll N_c$ so
whatever.

The collected data gives $h_{vaxxed} = 3.68$ per 100,000 people for that
first month of May.

For that week the nasty unvaxxed saw an $N_c$ about 10x larger and
$N_i$ is 100x larger so $h_{nasty} = 35.80$ per 100,000 is 10x more.

Then an intermediate /hazard ratio/ of those two functions, $HR = 0.103$
and finally the /vax effectiveness/ of 89.7%,

\[VE = 1-HR\]

To calculate the VE for the next week we do the same thing after
reducing the number of cohorts by how many got COVID during the
current week: $N^{w+1}_c = N^w_c - N^w_i$.

After all that, VE is simply and effectively (one minus) a double
ratio of the relative infected fractions between vaxxed and unvaxxed.

Now, knowing the definition it is clear to me that we are making a
small category error by attributing this measure of "effectiveness" to
just the mechanism of the vaccine.  There must be other, unknown (to
me at least), contributions that are correlated with being vaccinated.
For example, people that get vaxxed must also be more likely to wear
masks, socially distance and engage in other behavior that counters
the spread of the virus.  All those activities will keep their
relative infections lower than their nastier counterparts.

So what?  Well, not much.  Mostly I don't like so much attention payed
to undefined numbers.  But also, there is a potential curiosity of
statistics we are missing.  Faced with two choices, say two types of
vaccines, we might want to know more about these correlations to see
if they might break the tie for which choice is best for our personal
choice.  

Say, just for sake of example, Pfizer and Moderna had the same VE as
defined above.  Now let's pretend we knew that "Pfizer people" were
much more into masks and social distancing.  That would mean that
Moderna must be a better vaccine in order for the two to have an equal
VE.  We might then decide personally to get Modernal and mask up and
distance socially.  Again, just a fabricated example.  I'm not saying
one vax is better or the other.

And, I'm also definitely not saying don't get vaxxed.  Get vaxxed, you
filthy swine!
